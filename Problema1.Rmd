---
title: "Actividad 2 MSE Grupo 4"
author: "Sebastian Toro y Carlos Preciado"
date: "2025-09-13"
output:
  bookdown::html_document2:
    base_format: rmdformats::readthedown
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Descripci√≥n del Problema 1

Este problema se centra en el an√°lisis de un proceso de solicitudes de servicio t√©cnico que sigue una distribuci√≥n de Poisson, un modelo probabil√≠stico discreto ideal para contar el n√∫mero de eventos en un intervalo de tiempo fijo. El objetivo principal es explorar c√≥mo los conceptos de la inferencia estad√≠stica, como la estimaci√≥n de par√°metros y la convergencia de los estimaciones, se manifiestan en la pr√°ctica a trav√©s de la simulaci√≥n.

La tasa promedio de solicitudes (Œª) es de 5 por hora, lo que representa el par√°metro poblacional. A trav√©s de la simulaci√≥n, generaremos muestras aleatorias para observar c√≥mo los estimadores muestrales, en este caso la frecuencia relativa y la media muestral, se aproximan a sus respectivos par√°metros te√≥ricos (la probabilidad P(X=3) y la media Œª=5) a medida que el tama√±o de la muestra aumenta. Esto nos permitir√° ilustrar los principios de la consistencia y la Ley de los Grandes N√∫meros.

### Introducci√≥n: Marco Te√≥rico y Justificaci√≥n
El siguiente informe presenta un an√°lisis sobre el comportamiento de una distribuci√≥n de Poisson, aplicada a la tasa de solicitudes de reparaci√≥n de una empresa de servicio t√©cnico. La premisa es que el n√∫mero de solicitudes por hora sigue una distribuci√≥n de Poisson con una media te√≥rica (par√°metro Œª) de 5.

El objetivo de este estudio es doble:

- Estimaci√≥n de Probabilidad: Estimar la probabilidad de un evento espec√≠fico (X=3 solicitudes por hora) tanto de forma te√≥rica como a trav√©s de simulaciones.

- Estimaci√≥n de la Media: Analizar c√≥mo la media muestral se aproxima a la media poblacional (Œª=5).

Este ejercicio pr√°ctico nos permite ilustrar conceptos clave de la inferencia estad√≠stica, como la consistencia de un estimador y la Ley de los Grandes N√∫meros. Un estimador es consistente si su valor se aproxima al par√°metro real de la poblaci√≥n a medida que el tama√±o de la muestra crece. La Ley de los Grandes N√∫meros establece que el promedio de una muestra aleatoria de una variable aleatoria converge a la media te√≥rica de la poblaci√≥n a medida que el tama√±o de la muestra aumenta.

### An√°lisis y Resultados

### C√°lculo de Probabilidad Te√≥rica
La probabilidad de que lleguen exactamente 3 solicitudes en una hora, bajo una distribuci√≥n de Poisson con Œª=5, se calcula usando la f√≥rmula:

$$
P(X = x) = \frac{\lambda^x e^{-\lambda}}{x}
$$
 
Para $x = 3$ y $\lambda = 5$, la probabilidad es:

$$
P(X = 3) = \frac{5^3 e^{-5}}{3} = \frac{125 \times 0.006738}{6} \approx 0.1404
$$

En R, este c√°lculo se realiza con dpois(3, lambda = 5), lo que nos da el valor exacto de 0.1403738. Este es el valor de referencia con el que compararemos los resultados de nuestras simulaciones.

### Simulaci√≥n con una Muestra
Se gener√≥ una √∫nica muestra de tama√±o $n=1,000$ a partir de una distribuci√≥n de Poisson con $Œª=5$. El c√≥digo rpois(1000, lambda = 5) simula el n√∫mero de solicitudes en 1,000 horas diferentes.

El c√°lculo de la frecuencia relativa para $X=3$ en esta muestra se obtuvo como: 
$$\frac{numero\;de\;veces\;que\;aparecio\;3}{1,000} $$

La frecuencia relativa calculada fue de 0.151, muy cercana a la probabilidad te√≥rica de 0.1404 al desviarse en solo 0.0106. Esto demuestra que, incluso con una sola muestra relativamente grande, la frecuencia observada ya comienza a aproximarse a la probabilidad te√≥rica. Adicionalmente, queda en evidencia que El error de muestreo es una caracter√≠stica inherente a la estimaci√≥n basada en muestras. La diferencia entre el valor del estimador (0.151) y el par√°metro poblacional (0.1404) es una manifestaci√≥n directa de este error aleatorio.

El hecho de que la frecuencia observada sea ligeramente superior a la te√≥rica es perfectamente consistente con la naturaleza aleatoria del proceso de muestreo. Nuestro estimador, la frecuencia relativa, est√° dise√±ado para aproximarse al par√°metro, pero nunca se garantiza que sea exactamente igual en una √∫nica observaci√≥n muestral.

Podriamos decir que el valor de 0.151 es una estimaci√≥n fiable, ya que la desviaci√≥n es m√≠nima en el gran esquema de la distribuci√≥n de probabilidad, confirmando que la muestra de n=1,000 fue lo suficientemente representativa.

### An√°lisis de la Variabilidad entre Muestras

Para ilustrar el concepto de la distribuci√≥n muestral de un estimador, se generaron 100 muestras, cada una de tama√±o $n=1,000$, y se calcul√≥ la frecuencia relativa para $X=3$ en cada una.

```{r, echo=FALSE, fig.cap="Gr√°fico de dispersion de Frecuencia Relativa de X=3 en 100 Muestras.", label="frecuencia-variabilidad"}
knitr::include_graphics("graphs/frecuencia_relativa_100muestras.png")
```

El gr√°fico \@ref(fig:frecuencia-variabilidad) de dispersi√≥n con las 100 frecuencias relativas y la l√≠nea horizontal en la probabilidad te√≥rica de 0.1404 muestra lo siguiente:

- **Centro y Tendencia:** Los puntos se agrupan alrededor de la l√≠nea horizontal. Como puede observarse, la gran mayor√≠a de los puntos (cada uno representando la frecuencia relativa de una muestra) se agrupan de manera clara y visible alrededor de la l√≠nea horizontal de referencia. Esta l√≠nea representa la probabilidad te√≥rica de $P(X=3)‚âà0.1404$. La concentraci√≥n de puntos en este valor nos demuestra, de forma visual, la propiedad de **insesgadez**. Esto significa que, aunque cualquier muestra individual puede tener un valor ligeramente diferente (por ejemplo, el 0.151 del an√°lisis anterior o el punto cerca de 0.17), el promedio de las estimaciones en el largo plazo (las 100 muestras en este caso) converge al valor real.

- **Dispersi√≥n y Precisi√≥n:** No hay una tendencia de los puntos a desviarse por encima o por debajo de la l√≠nea. Y aunque, los puntos no est√°n perfectamente alineados, lo que confirma la presencia del **error de muestreo** inherente. Sin embargo, la dispersi√≥n es relativamente baja. La mayor√≠a de los puntos se encuentran en un rango estrecho, lo que indica que, para un tama√±o muestral de n=1,000, nuestro estimador es **preciso**. De modo que incluso si solo tomamos una muestra, es muy probable que nuestra estimaci√≥n est√© muy cerca del valor real. La precisi√≥n es una medida de la fiabilidad de la estimaci√≥n.

### Impacto del Tama√±o Muestral En la Frecuencia Relativa

En esta secci√≥n, se generaron muestras de tama√±os crecientes (desde 5 hasta 1,000) para observar el efecto de la muestra en la estimaci√≥n.

```{r 3, echo=FALSE, fig.cap="Frecuencia relativa creciente en funci√≥n del tama√±o muestral"}
knitr::include_graphics("graphs/frecuencia_relativa_creciente.png")
```

El gr√°fico de la **Frecuencia Relativa vs. Tama√±o Muestral** es una de las demostraciones visuales m√°s poderosas de la **consistencia** de un estimador y de la **Ley de los Grandes N√∫meros**.

* **Fase de Alta Variabilidad (Tama√±os Muestrales Peque√±os):** Lo primero que notamos es la volatilidad de las estimaciones para tama√±os de muestra peque√±os (n=5, 10, 20). Los puntos se alejan significativamente de la l√≠nea te√≥rica de $P(X=3) \approx 0.1404$. Esto subraya un principio cr√≠tico en la pr√°ctica: las conclusiones basadas en datos limitados pueden ser altamente enga√±osas y propensas a un **error de muestreo** considerable. Una estimaci√≥n de 0.20 (para n=5), por ejemplo, no es at√≠pica, pero carece de la fiabilidad necesaria para una toma de decisiones informada.

* **Fase de Convergencia (Tama√±os Muestrales Grandes):** A medida que el tama√±o de la muestra aumenta (a partir de n=100 en el gr√°fico), se observa una tendencia inconfundible. La variabilidad de los puntos se reduce dr√°sticamente, y las estimaciones se "pegan" a la l√≠nea de la probabilidad te√≥rica. Este comportamiento de **convergencia** es la garant√≠a de que, si tenemos suficientes datos, nuestra estimaci√≥n muestral se aproximar√° cada vez m√°s al verdadero par√°metro de la poblaci√≥n.

Esto demuestra de forma visual el concepto de **consistencia**: a medida que el tama√±o de la muestra tiende a ser m√°s grande, la frecuencia relativa (nuestro estimador) converge hacia la probabilidad te√≥rica (el par√°metro real). En otras palabras, la diferencia absoluta entre el estimador y el par√°metro tiende a cero.

### Convergencia de la Media Muestral

Este an√°lisis es similar al anterior, pero centrado en la media de la distribuci√≥n. La media te√≥rica de una distribuci√≥n de Poisson es igual a su par√°metro Œª, en este caso, 5.


```{r 4, echo=FALSE, fig.cap="Grafico de Dispersion Promedio Muestral en 100 Muestras"}
knitr::include_graphics("graphs/dispersion_prm_muestral.png")
```

El gr√°fico que muestra los **Promedios Muestrales en 100 Muestras** de tama√±o $n=1,000$ es una representaci√≥n visual de la **distribuci√≥n muestral de la media**.

* **Evidencia de Insesgadez y Precisi√≥n:** El patr√≥n de los puntos es notablemente similar al de las frecuencias relativas. Los promedios se distribuyen alrededor de la media te√≥rica ($\lambda = 5$), lo que refuerza que la media muestral es un **estimador insesgado**. La baja dispersi√≥n de los puntos (la mayor√≠a entre 4.9 y 5.1) para un tama√±o de muestra de $n=1,000$ demuestra la **alta precisi√≥n** del estimador. Un bajo **error est√°ndar** de la media muestral nos indica que, si repiti√©ramos el experimento, los resultados estar√≠an muy cerca del promedio te√≥rico.

* **El Teorema del L√≠mite Central en Acci√≥n:** Si bien este gr√°fico no muestra la distribuci√≥n de forma expl√≠cita, la agrupaci√≥n sim√©trica de los puntos alrededor de la media te√≥rica es un resultado directo del **Teorema del L√≠mite Central (TLC)**. El TLC garantiza que, para un tama√±o muestral suficientemente grande, la distribuci√≥n de los promedios muestrales se aproxima a una distribuci√≥n normal, independientemente de la forma de la distribuci√≥n original de la poblaci√≥n. Esta normalidad nos permite realizar inferencias y construir intervalos de confianza, que son herramientas clave en la estad√≠stica.


### Impacto del Tama√±o Muestral en la Media

Finalmente, se investig√≥ c√≥mo el tama√±o de la muestra afecta la convergencia de la media muestral.


```{r 5, echo=FALSE, fig.cap="Grafico de Promedios Muestrales"}
knitr::include_graphics("graphs/promedios_muestrales.png")
```


Este gr√°fico, que muestra la **Convergencia del Promedio Muestral**, es la otra cara de la moneda de la Ley de los Grandes N√∫meros.

* **Del Caos a la Estabilidad:** La trayectoria del promedio muestral desde n=5 hasta n=1,000 es elocuente. Para muestras peque√±as, el promedio muestral es err√°tico, con fluctuaciones salvajes. Por ejemplo, se observan valores que var√≠an desde 4.4 a 5.4. Esto subraya que la variabilidad de la muestra inicial puede llevar a estimaciones muy diferentes y potencialmente enga√±osas.

* **La Estabilizaci√≥n del Estimador:** Sin embargo, a medida que el tama√±o muestral aumenta, la l√≠nea se estabiliza y se pega a la l√≠nea horizontal de $\lambda = 5$. Esto es la esencia de la Ley de los Grandes N√∫meros: el "promedio" de las observaciones aleatorias converge a su valor esperado a medida que la muestra crece. Como estad√≠stico, este gr√°fico es la justificaci√≥n fundamental para el trabajo con grandes conjuntos de datos (Big Data), ya que nos asegura que, al acumular m√°s informaci√≥n, la estimaci√≥n del par√°metro poblacional se vuelve m√°s robusta, fiable y menos susceptible a las variaciones aleatorias de un peque√±o n√∫mero de observaciones.

Este comportamiento ilustra el funcionamiento de la **Ley de los Grandes N√∫meros:** el promedio de las observaciones de una muestra aleatoria de un proceso tiende a la media te√≥rica (el valor esperado de la poblaci√≥n) a medida que el tama√±o de la muestra se hace m√°s grande. Esto es un pilar fundamental de la inferencia estad√≠stica y valida el uso de la media muestral como un estimador confiable del par√°metro poblacional en este caso

## Referencias

- Casella, G., & Berger, R. L. (2002). *Statistical Inference* (2nd ed.). Duxbury.  
- Wasserman, L. (2004). *All of Statistics: A Concise Course in Statistical Inference*. Springer.  
- Rice, J. A. (2007). *Mathematical Statistics and Data Analysis* (3rd ed.). Cengage.  
- Ross, S. (2019). *Introduction to Probability and Statistics for Engineers and Scientists* (6th ed.). Academic Press.  
- Montgomery, D. C., & Runger, G. C. (2018). *Applied Statistics and Probability for Engineers* (7th ed.). Wiley.  
- Agresti, A., & Franklin, C. (2017). *Statistics: The Art and Science of Learning from Data* (4th ed.). Pearson.  
- DeGroot, M. H., & Schervish, M. J. (2012). *Probability and Statistics* (4th ed.). Addison-Wesley.  
- Blitzstein, J. K., & Hwang, J. (2019). *Introduction to Probability*. Chapman & Hall/CRC.  
- Downey, A. (2012). *Think Stats: Exploratory Data Analysis in Python*. O‚ÄôReilly. (Disponible gratis en l√≠nea).  
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Applications in R* (2nd ed.). Springer.  
  üëâ [https://www.statlearning.com](https://www.statlearning.com)  
- OpenIntro. (2021). *OpenIntro Statistics* (4th ed.). OpenIntro Project.  
  üëâ [https://www.openintro.org/book/os/](https://www.openintro.org/book/os/)
