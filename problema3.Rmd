---
title: "Actividad 2 MSE Problema 3"
author: "Sebastian Toro y Carlos Preciado"
date: "2025-09-26"
output:
  bookdown::html_document2:
    base_format: rmdformats::readthedown
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Descripci√≥n del Problema 3

El Problema 3 se enfoca en el Teorema del L√≠mite Central (TLC), una de las ideas m√°s fundamentales de la estad√≠stica inferencial. El contexto es el tiempo de funcionamiento continuo de una m√°quina en una planta, el cual se modela mediante una distribuci√≥n Exponencial con un par√°metro de tasa $Œª=0.16$.

Para este caso contamos con el hecho de que la distribuci√≥n Exponencial es intr√≠nsecamente sesgada, lo que la convierte en el ejemplo perfecto para demostrar el poder del TLC.

As√≠ mismo, las actividades de simulaci√≥n buscan responder a la pregunta crucial: ¬øQu√© tan grande debe ser una muestra (n) para que la distribuci√≥n de la media muestral se comporte como una distribuci√≥n Normal, permitiendo as√≠ realizar inferencias param√©tricas v√°lidas sobre el tiempo de vida promedio de la m√°quina?

Para lograrlo, se realizar√°n tres tareas principales:

- Establecer los par√°metros poblacionales (media y varianza te√≥ricas) del tiempo de vida.

- Verificar el TLC mediante la simulaci√≥n de 100 medias muestrales con un n fijo $(n=200)$, evaluando la convergencia de la media y la varianza observada hacia los valores te√≥ricos.

- Determinar el umbral de n necesario para alcanzar la normalidad, variando el tama√±o muestral de n=5 a n=2,000 y aplicando un test de normalidad.

Este an√°lisis es la base de todo el mantenimiento predictivo, ya que nos dice bajo qu√© condiciones podemos utilizar la fiabilidad de la curva Normal para predecir el comportamiento del sistema.

### C√°lculo de la Media y Varianza de la Poblaci√≥n

Del enunciado podemos sacar las siguientes notas: la distribuci√≥n Exponencial, que modela el tiempo de funcionamiento continuo, tiene un par√°metro de tasa Œª=0.16. Y, por teor√≠a, sabemos que los par√°metros poblacionales se calculan te√≥ricamente de la siguiente manera:

$$
Œº = \frac{1}{\lambda} = \frac{1}{0.16} = 6.25\;meses
$$

Del mismo modo, por teor√≠a, sabemos que la varianza poblacional:

$$
œÉ^2 = \frac{1}{\lambda^2} = \frac{1}{0.16^2} = 39.0625\;meses^2
$$

Es decir que, el tiempo esperado antes de que la m√°quina requiera mantenimiento es de 6.25 meses. La alta varianza (39.0625) en relaci√≥n con la media indica una variabilidad significativa en los tiempos de funcionamiento. Esto es t√≠pico de la distribuci√≥n Exponencial, que carece de memoria, lo que significa que la dispersi√≥n de los posibles tiempos de fallo es considerablemente amplia. Esto tambi√©n podr√≠a ser interpretado como que las m√°quinas no dan una sensaci√≥n de seguridad, sino por el contrario de incertidumbre. Pues no hay una forma determin√≠stica de saber si la m√°quina esta pr√≥xima a fallar o no. Pero esto algo que por el momento no es del todo seguro, de modo que pasaremos a usar ayudas gr√°ficas para poder tener m√°s informaci√≥n al respecto.

### Gr√°fico de la Curva de Densidad

```{r, results='asis', echo = FALSE, message = FALSE, warning = FALSE}
lambda <- 0.16
curve(dexp(x, rate = lambda), from = 0, to = 30, col = "darkblue", lwd = 2, 
      main = "Funci√≥n de Densidad Exponencial (Œª=0.16)",
      xlab = "Tiempo de Funcionamiento (meses)",
      ylab = "Densidad de Probabilidad")
abline(v = 1/lambda, col = "red", lty = 2)
legend("topright", legend = c("Media Poblacional (6.25)"), col = "red", lty = 2, cex = 0.8)
```

La Figura anterior muestra la forma caracter√≠stica de la distribuci√≥n Exponencial: sesgada positivamente (a la derecha). El par√°metro Œª (la tasa de fallos) es inversamente proporcional a la media. Un Œª m√°s grande mover√≠a la curva m√°s hacia el origen, indicando una vida √∫til promedio incluso a√∫n m√°s corta y una mayor probabilidad de fallos prematuros. Un Œª m√°s peque√±o mover√≠a la curva hacia la derecha, indicando un mayor tiempo de funcionamiento promedio.

El modelo indica que la probabilidad de un fallo prematuro (tiempos cercanos a cero) es la m√°s alta, lo cual se ve en la densidad m√°xima en $t=0$. Esto subraya que la m√°quina es m√°s probable que falle pronto que tarde. Es por esta raz√≥n que mencionabamos en el anterior punto que no solo la teor√≠a era suficiente para poder sacar conclusiones, tambi√©n era necesario simular la curva de densidad para poder determinar gr√°ficamente como se distribuyen los datos. Gracias a esto, podemos decir que el mantenimiento predictivo deber√≠a centrarse en monitorear la m√°quina durante los primeros 6 a 8 meses para gestionar la alta variabilidad y evitar fallos costosos.

### Comparaci√≥n de Par√°metros, Estimadores y Estimaciones (n=200)

```{r 1, echo=FALSE, fig.cap="Histograma de una de las 10 Muestras Simuladas"}
knitr::include_graphics("graphs/hist_muestra1_n200.png")
```

La Figura 1 muestra el histograma de una de las 10 muestras. A pesar de que el tama√±o de la muestra es considerable (n=200), la distribuci√≥n de los datos individuales sigue siendo la poblaci√≥n de origen (Exponencial), con el caracter√≠stico sesgo a la derecha. Esto es fundamental: la muestra individual refleja la forma de la poblaci√≥n, no del TLC.

Pero recordemos que lo que garantiza el TLC es que la distribuci√≥n de la media muestral, es decir, si tomamos muchas muestras independientes y calculamos sus medias, tender√° a ser Normal cuando 
ùëõ
n sea suficientemente grande.

Adicionalmente, podemos ver que la poblaci√≥n tiene distribuci√≥n Exponencial, que se caracteriza por:
- fuerte sesgo positivo (cola larga hacia la derecha),
- moda cerca de cero,
- media mayor que la mediana.

Tambi√©n es pertinente comentar que, Aunque $ùëõ= 200$ es grande, no es suficiente para que la distribuci√≥n de una sola muestra se "suavice" hacia la Normal. Esto se debe a que el TLC act√∫a sobre la media, no sobre los valores brutos.

Dicho de otra manera:
- La forma de la poblaci√≥n ‚Üí se refleja en cualquier muestra.
- La forma de la distribuci√≥n de las medias muestrales ‚Üí depende de ùëõy del TLC.

Al revisar los c√°lculos obtenidos en la secci√≥n c de los c√≥digos podemos observar lo siguiente para una simulaci√≥n con la semilla 123:

```{r 2, echo=FALSE, fig.cap="Medias Muestrales para 10 Muestras con N=200"}
knitr::include_graphics("graphs/resultados_medias_muestrales.png")
```

Bas√°ndonos en loos valores poblacionales reales son $Œº=6.25$ y $œÉ^2=39.0625$. Estos son fijos y desconocidos en un escenario real. Y teniendo en cuenta que las f√≥rmulas utilizadas para aproximar estos par√°metros son:

- La media del Estimador: 
$$
\hat{\mu} = \bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i \quad \text{(La media muestral)}
$$

- Estimador de la Varianza:
$$
\hat{\sigma}^2 = S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \quad \text{(La varianza muestral insesgada)}
$$

Podemos decir que, dentro de los valores num√©ricos obtenidos de las 10 muestras (ej. 6.321 y 36.512 para la Muestra 1). Las estimaciones son muestrales y aleatorias. Su proximidad a los valores poblacionales es una evidencia de que los estimadores $\bar{X}$ y $S^2$ son eficientes y consistentes, incluso con una poblaci√≥n fuertemente sesgada.

### Aplicaci√≥n del Teorema del L√≠mite Central para n=200

```{r 3, echo=FALSE, fig.cap="Visualizaci√≥n del TLC a trav√©s de un Histograma de Medias Muestrales"}
knitr::include_graphics("graphs/hist_medias_n200.png")
```

La Figura 3 ilustra el poder del Teorema del L√≠mite Central (TLC).

```{r, results='asis', echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(digits = 6)
library(knitr)
library(kableExtra)

# Definir tabla con expresiones LaTeX
df <- data.frame(
  Estad√≠stica = c(
    "Media de las Medias ($E[\\bar{X}]$)", 
    "Varianza de las Medias ($\\sigma^2_{\\bar{X}}$)",
    "Test de Normalidad (p-valor)"
  ),
  `Valor Poblacional Te√≥rico` = c("6.250", "0.1953", "-"),
  `Valor Obtenido de 100 Medias` = c("6.241", "0.1887", "0.897"),
  Comparaci√≥n = c("Convergencia a $\\mu$", 
                  "Convergencia a $\\sigma^2/n$", 
                  "Normalidad aceptada")
)

# Renderizar con kableExtra en HTML
kable(df, format = "html", escape = FALSE,
      caption = "Tabla de Estad√≠sticas Comparadas") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  )
```

Evidencia del Teorema del L√≠mite Central:

- Distribuci√≥n de las Medias: El histograma de las 100 medias muestrales (n=200) es perfectamente sim√©trico y acampanado, a pesar de que la poblaci√≥n original es Exponencial (sesgada). Este cambio de forma es la manifestaci√≥n visual m√°s clara del TLC.

Media y Varianza:

- El promedio de las 100 medias (6.241) converge muy cerca del valor poblacional (6.25), demostrando que la media muestral es un estimador insesgado.

- La varianza de las medias (0.1870) es muy pr√≥xima a la varianza te√≥rica del TLC 
$$\\sigma^2/n = 0.1953$$.  
Esto prueba la **ley de la ra√≠z cuadrada**: el error est√°ndar de la media disminuye en proporci√≥n a $$\frac{1}{\sqrt{n}}$$.

Test de Normalidad: El p-valor (0.8831) del test de Shapiro-Wilk es mayor que Œ±=0.05. Por lo tanto, no tenemos suficiente evidencia para rechazar la hip√≥tesis nula de que la distribuci√≥n de las medias muestrales es Normal.

El TLC se valida: la distribuci√≥n muestral de la media se aproxima a la Normal, se centra en la media poblacional y tiene una varianza predecible 
$$
\frac{\sigma^2}{n}
$$

### Aplicaci√≥n del Teorema del L√≠mite Central variando n

```{r 4, echo=FALSE, fig.cap="Visualizaci√≥n del TLC a trav√©s de un Histograma de Medias Muestrales"}
knitr::include_graphics("graphs/convergencia_varianza_tlc.png")
```

El an√°lisis de las 100 muestras al variar el tama√±o n nos permite observar el ritmo de la convergencia hacia la normalidad y la reducci√≥n de la varianza.

```{r echo=FALSE, message=FALSE}
library(knitr)
library(kableExtra)

# Datos de la tabla
tabla <- data.frame(
  n = c(5, 10, 80, 200, 500, 2000),
  "Promedio Medias" = c(6.262, 6.248, 6.247, 6.273, 6.244, 6.257),
  "Varianza Observada" = c(8.01, 3.82, 0.49, 0.20, 0.07, 0.02),
  "Varianza Te√≥rica" = c(7.8125, 3.9062, 0.4883, 0.1953, 0.0781, 0.0195),
  "p-valor Normalidad" = c("0.000 (No Normal)", 
                           "0.002 (No Normal)", 
                           "0.511 (Normal)", 
                           "0.992 (Normal)", 
                           "0.999 (Normal)", 
                           "0.998 (Normal)")
)

# Renderizar la tabla en HTML bonito
kable(tabla, format = "html", booktabs = TRUE, align = "c",
      caption = "Resultados de la simulaci√≥n para diferentes tama√±os de muestra") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE, font_size = 14)
```

Conclusi√≥n sobre la Validez del TLC:

- Comportamiento de la Distribuci√≥n:
  - Para n=5 y n=10, los histogramas de las medias muestrales a√∫n muestran cierto sesgo, y el test de normalidad rechaza la hip√≥tesis Normal (p-valor <0.05). Esto se debe a que la poblaci√≥n de origen es fuertemente Exponencial; se necesita un n mayor para superar su sesgo.
  - A partir de n=80, el TLC se vuelve completamente v√°lido. Los p-valores suben dr√°sticamente (ej., 0.511 y superiores), indicando que la distribuci√≥n de las medias es, a efectos pr√°cticos, Normal.
  
- Convergencia de la Varianza:
  - La Figura @ref(fig:convergencia_varianza_tlc) es la prueba de la convergencia. La l√≠nea de la Varianza Observada sigue casi perfectamente la l√≠nea de la Varianza Te√≥rica ($\frac{\sigma^2}{n}$). A medida que n aumenta, ambas var√≠an de forma id√©ntica y convergen a cero. Esto valida el uso del Error Est√°ndar para construir intervalos de confianza.
  
En conclusi√≥n podemos decir lo siguiente, en el contexto del mantenimiento predictivo, el TLC nos permite utilizar la distribuci√≥n Normal para hacer inferencias sobre el tiempo de vida promedio de la m√°quina, siempre y cuando la muestra de la que se extrae la media sea de n‚â•80. Para n peque√±os, cualquier inferencia debe hacerse con cautela o utilizando m√©todos no param√©tricos.

## Referencias

- Casella, G., & Berger, R. L. (2002). *Statistical Inference* (2nd ed.). Duxbury.  
- Wasserman, L. (2004). *All of Statistics: A Concise Course in Statistical Inference*. Springer.  
- Rice, J. A. (2007). *Mathematical Statistics and Data Analysis* (3rd ed.). Cengage.  
- Ross, S. (2019). *Introduction to Probability and Statistics for Engineers and Scientists* (6th ed.). Academic Press.  
- Montgomery, D. C., & Runger, G. C. (2018). *Applied Statistics and Probability for Engineers* (7th ed.). Wiley.  
- Agresti, A., & Franklin, C. (2017). *Statistics: The Art and Science of Learning from Data* (4th ed.). Pearson.  
- DeGroot, M. H., & Schervish, M. J. (2012). *Probability and Statistics* (4th ed.). Addison-Wesley.  
- Blitzstein, J. K., & Hwang, J. (2019). *Introduction to Probability*. Chapman & Hall/CRC.  
- Downey, A. (2012). *Think Stats: Exploratory Data Analysis in Python*. O‚ÄôReilly. (Disponible gratis en l√≠nea).  
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Applications in R* (2nd ed.). Springer.  
  üëâ [https://www.statlearning.com](https://www.statlearning.com)  
- OpenIntro. (2021). *OpenIntro Statistics* (4th ed.). OpenIntro Project.  
  üëâ [https://www.openintro.org/book/os/](https://www.openintro.org/book/os/)